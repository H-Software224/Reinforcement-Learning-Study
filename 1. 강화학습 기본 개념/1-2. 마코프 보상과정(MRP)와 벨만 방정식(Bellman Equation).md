## 목차
1. [마르코프 보상 과정(MRP, Makov Reward Process)](#1.-마르코프-보상-과정(MRP,-Makov-Reward-Process))
2. [상태 가치 함수(v : State Value Function)](#2.-상태-가치-함수(v-:-State-Value-Function))
3. [벨만 방정식(Bellman Equation)](#3.-벨만-방정식(Bellman-Equation))

<br>

# 1. 마코프 보상 과정(MRP, Makov Reward Process)

마코프 보상 과정(MRP)이란, 마코프 연쇄에 **보상(Reward)** 과 시간에 따른 보상의 감가율인 **감마($\gamma$)** 가 추가된 개념이다.
마코프 연쇄는 상태에 전이확률만 주어지니, 상태변화가 얼마나 가치있는지는 알 수 없다. 반면 MRP는 상태변화의 가치까지 계산한다.

## MRP의 구성요소

### 1. **상태집합($S$)**

상태 $S$ 는 에이전트가 관찰 가능한 상태의 집합이다. 상태라는 것은 에이전트가 갈 수 있는 상황이라고 생각하면 쉽다. 뒤에서 나올 그리드월드 예제에서의 상태 집합은 각 좌표이다. 
어떤 시점 $t$에서 상태 $S_t$ 는 정해져 있지 않다. MDP에서 상태는 시간에 따라 확률적으로 변한다.

MRP에서는 상태는 유한하다.
   
### 2. **상태 전이 매트릭스($P$)**

*상태 전이 매트릭스* 또는 *상태 변환 확률*, *환경의 모델* 이라고도 부른다.
수식은 **시간 $t$에서 상태가 $s$ 였을 때, 시간 $t+1$ 에서 상태가 $s'$ 로 변할 확률** 을 의미한다. 이 값은 에이전트가 알지 못하는 값으로 환경의 일부이다.

$P_{ss'} = P[S_{t+1} = s'|S_t = s]$
      
### 3. **보상함수($R$, Reward Function)**

보상은 에이전트가 학습하는 정보로, 환경이 에이전트에게 주는 정보이다. 에이전트가 어떤 행동을 했을 때 받을 보상을 수식을 나타내면 아래와 같다.
     
$R_s^a = E[R_{t+1}| S_t = s, A_t = a]$ 

즉, 시간 $t$에서 상태가 $s$ 였을 때 $a$ 행동을 할 경우 시간 $t+1$ 에서 받는 보상의 기댓값을 의미한다. 위 상태 전이 매트릭스에서 봤듯이, 에이전트가 a라는 행동을 하더라도 항상 똑같은 상태로 이동하지는 않는다. 환경에 영향을 따라 확률적으로 다른 상태로 이동하는 것이기 때문에 이를 기댓값으로 표현하는 것이다.

보상함수에서 주의해서 보아야 할 점은, 행동은 $t$ 시점에 하지만 보상을 받는 건 $t+1$ 이다. 에이전트가 보상을 알고 있지 않고, $t+1$ 시점에 이동한 그 상태를 보고 환경이 에이전트에게 보상을 주는 것이기 때문이다. 따라서 $R_{t+1}$ 와 같이 표현한다. 
        

아래 보상함수를 계산하는 예시를 보자.

<img src="https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/726c42cd-af38-4585-be46-ffbaf4d4b4f3" width="300" height="200">

상태 $s$가 $t+1$에서 가질 수 있는 상태가 2개($S2, S3$)이고 각각으로 갈 확률이 $p1, p2$이며, 각 상태변화에 따른 보상이 $r1, r2$ 일 때 보상함수는 아래와 같다.
        
$$R_{s1} = p1\times r_1 + p_2 \times r_2$$
        
확률을 곱해 기댓값 형태로 나타냈다. 이때 중요한 것은 상태 $s1$의 보상은 t+1에서 계산된다는 점이다.
  
### 4. **감가율, 할인율($\gamma$)**

감가율 또는 할인율이라고 부른다. 가까운 보상일수록 더 큰 가치를 주기 위해 사용한다. 10년 뒤에 1억을 받는 것과 지금 당장 1억을 받는 것에는 차이가 있다. 현재가치로 환산해서 비교해야 한다. 이때 사용하는 것이 감가율(할인율)이다. 미래에 받을 보상을 현재의 가치로 환산한다면 그 크기가 작아질 것이다. 할인율 $\gamma$는 0와 1사이의 값을 가지기 때문이다. 감가율이 0이면 미래의 보상을 전혀 고려하지 않는 것이고, 감가율이 1이면 현재와 미래의 가치를 동일하게 보는 것이다.

$\gamma \in [0, 1]$

만약 현재 시간인 $t$로부터 시간 $k$가 지난 후에 보상을 $R_{t+k}$ 만큼 받는다면, 이를 현재가치로 환산하면 어떻게 될까? 현재로부터 시간이 k만큼 지났기 때문에 미래에 받을 보상은 $\gamma^(k-1)$ 만큼 할인된다. 따라서 할인율을 고려한 보상의 현재 가치는 $\gamma^(k-1)R_{t+k}$ 이다.


<br>

# 2. 상태 가치 함수(v : State Value Function)

이렇게 MRP의 구성요소를 알아보았다. 강화학습의 목적은 보상을 최대화 하는 것이다. 이것을 알기 위해 "가치 함수"를 사용한다.

현재 어떤 상태에 있고, 앞으로 받을 보상들을 고려해서 좋은 선택을 해야 한다. 그 아직 받지 않은 보상들을 더해서 이를 최대가 되도록 하는 행동을 하면 된다. 그러나 앞서 '감가율' 개념에서 봤듯이 미래의 보상을 단순 합하지 않고, 감가율을 사용해서 더해준다. 이렇게 하는 이유는 단순 보상의 합으로는 시간 $t$에서의 상태가 어떤 가치를 가지는지 판단하기 어렵기 때문이다. 미래의 보상을 현재의 값으로 표현하기 위해 감가율($\gamma$)을 사용하여 더해준 것을 **반환값($G_t$)** 이라고 한다.

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...
$$

에이전트는 환경과 유한한 시간동안 상호작용을 하고 마지막 상태가 되면 반환값을 계산한다. 
즉, 그때로부터 얼마의 보상을 받았는지를 아래처럼 계산하는 것이다.

$$\begin{aligned}
G_1&=R_2+\gamma R_3+\gamma^2 R_4+\gamma^3 R_5+\gamma^4 R_6\\
G_2&=R_3+\gamma R_4+\gamma^2 R_5+\gamma^3 R_6\\
G_3&=R_4+\gamma R_5+\gamma^2 R_6\\
G_4&=R_5+\gamma R_6\\
G_5&=R_6
\end{aligned}$$

이렇게 얻은 반환값으로 각 상태의 가치를 어떻게 알 수 있을까? 환경은 확률적이므로 여러 번의 실행(에피소드) 마다 반환값이 다를 수 있다. 따라서 상태의 가치를 나타낼 때는 기댓값을 사용한다.

$$v(s) = E[G_t|S_t =s]$$

이렇게 각 상태의 가치를 계산하는 이유는 에이전트가 갈 수 있는 상태들의 가치를 안다면 그중에서 가장 가치가 큰 곳으로 선택할 수 있기 때문이다.

$$\begin{aligned}
v(s) &= E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ... | S_t = s]\\
 &= E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3})|S_t = s]\\
 &= E[R_{t+1} + \gamma G_{t+1}|S_t = s]
\end{aligned}$$

가치 함수를 반환값을 통해 표현했다. $G_{t+1}$ 는 실제로 받을 보상이 아닌 기댓값이므로 이 부분을 가치함수로 바꿔서 쓸 수 있고, 아래와 같이 표현할 수 있다.

$$v(s) = E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]$$


이제 에피소드에서 반환값을 구하는 예시를 보자. 감가율을 0.5로 설정했다고 하자. 3타임스텝에 목적지에 도달하는 에피소드는 모두 3가지이며, 각 노드마다 보상이 정해져 있고, 이 보상값에 타임스텝이 진행됨에 따라 감가율을 계속 곱해주면 된다.

<img src="https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/d825284f-f63b-46ee-a23d-5bb5d83c00af" width="500" height="300">

<br>

# 3. 벨만 방정식(Bellman Equation)

벨만 방정식은 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계를 나타낸다.
위에서 본 상태가치함수를 수식을 좀 더 일반화하여 프로그래밍이 가능한 형태로 만든 것이다.
이를 **벨만 방정식(Bellman Equation)** 이라고 한다. 
강화학습에서는 프로그래밍으로 가치를 구하기 위해 이 방정식을 많이 사용한다. 

벨만 방정식은 **일반적으로 기댓값을 시그마 기호를 사용한 수열의 합으로 표현하며 현재상태와 다음상태의 관계**로 나타낸다. 

$$
\begin{aligned}
v(s) &= E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]&..1\\ 
&=R_{t+1} + \gamma E[v(S_{t+1})|S_t =s]&..2\\ 
&= R_{t+1} + \gamma\sum_{s' \in S}P_{ss'}v(s')&..3
\end{aligned}$$

1. 앞에서 본 상태가치함수
2. 상수를 기댓값에서 빼줌
3. 기댓값을 수열의 합과 다음 상태에서의 상태 가치 함수로 나타낸 것.

이 마지막 식을 **벨만 방정식**이라 부른다.

![타임스텝별로 고려해야 하는 상태](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/c0423288-be65-4c8b-98ee-5c4e51ac6564)


위와 같은 상황에서 벨만 방정식을 그대로 적용해 직접 가치를 계산하는 것은 쉬운 일이 아니다. 벨만 방정식을 사용하는 이유는 프로그래밍을 통해 문제를 해결하기 위함이다.

간단한 네트워크로 구성된 라우팅 문제의 경우 순차적으로 반환값을 구해 상태가치함수를 계산할 수 있다.
