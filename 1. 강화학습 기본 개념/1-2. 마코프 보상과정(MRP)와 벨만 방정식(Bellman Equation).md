## 목차
1. [마르코프 보상 과정(MRP, Makov Reward Process)](#1.-마르코프-보상-과정(MRP,-Makov-Reward-Process))
2. [벨만 방정식(Bellman Equation)](#2.-벨만-방정식(Bellman-Equation))



# 1. 마코프 보상 과정(MRP, Makov Reward Process)

마코프 보상 과정(MRP)이란, 마코프 연쇄에 **보상(Reward)** 과 시간에 따른 보상의 감가율인 **감마($\gamma$)** 가 추가된 개념이다.
마코프 연쇄는 상태에 전이확률만 주어지니, 상태변화가 얼마나 가치있는지는 알 수 없다. 반면 MRP는 상태변화의 가치까지 계산한다.

### MRP의 구성

1. **상태집합($S$)**
   
   MRP에서는 상태는 유한하다.
   
3. **상태 전이 매트릭스($P$)**
   
   $P_{ss'} = P[S_{t+1} = s'|S_t = s]$
      
4. **보상함수($R$, Reward Function)**
        
   $R_s = E[R_{t+1}|S_t =s]$  : 시간 t에서 상태가 s일 때, 시간 t+1에서 받을 수 있는 보상의 기댓값이다.

   이처럼 보상함수는 확률의 기댓값 형태로 표현할 수 있다. 아래 보상함수를 계산하는 예시를 보자.

   <img src="https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/726c42cd-af38-4585-be46-ffbaf4d4b4f3" width="300" height="200">

     상태 $s$가 $t+1$에서 가질 수 있는 상태가 2개($S2, S3$)이고 각각으로 갈 확률이 $p1, p2$이며, 각 상태변화에 따른 보상이 $r1, r2$ 일 때 보상함수는 아래와 같다.
              
     $$R_{s1} = p1\times r_1 + p_2 \times r_2$$
              
     확률을 곱해 기댓값 형태로 나타냈다. 이때 중요한 것은 상태 $s1$의 보상은 t+1에서 계산된다는 점이다.
  
6. **감가율, 할인율($\gamma$)**

    $\gamma \in [0, 1]$
    
   MRP의 목적은 가치를 계산하는 것인데, 이 가치는 보상함수를 사용해 하나의 에피소드 혹은 전체 환경의 가치를 한꺼번에 모두 계산한다.
   그리고 그 가치는 현재가치로 환산되어야 한다. 여러 번의 타임스텝 후에 얻게 되는 현재가치를 환산하는데,
   여기에 감가율을 사용하는 것이다. 이자율을 생각해보면 된다. 
   감가율이 0이면 미래의 보상을 전혀 고려하지 않는 것이고, 감가율이 1이면 현재와 미래의 가치를 동일하게 보는 것이다.
        



### **반환값**

여기에서 **반환값($G$: return)** 이라는 개념이 등장한다. 
**반환값은 타임스텝 t에서 계산한 누적 보상의 합계**이다.
반환값은 주로 전체 환경이 아닌 에피소드 단위로 계산하는데, 에피소드의 효율성이나 가치를 반환값을 통해 평가하며,
이 반환값을 극대화할 수 있도록 환경을 설계하는 것이 MRP의 목적중 하나다.

$$
G_t = R_{t+1} + rR_{t+2} + r^2R_{t+3} ... = \sum_{k=0}^\inf r^kR_{t+k+1}
$$

여기에서 특이한 점은 상태전이확률을 고려하지 않는다는 점이다. 반환값은 하나의 선택된 경로(에피소드)에 대한 전체적인 보상을 계산하는 방법이기 때문이다. 
(= 이미 경로가 결정된 상태에서 계산하는 게 반환값. 확률은 필요 없다.)

이제 반환값을 구하는 예시를 보자. 감가율을 0.5로 설정했다고 하자. 3타임스텝에 목적지에 도달하는 에피소드는 모두 3가지이며, 각 노드마다 보상이 정해져 있고, 이 보상값에 타임스텝이 진행됨에 따라 감가율을 계속 곱해주면 된다.

<img src="https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/d825284f-f63b-46ee-a23d-5bb5d83c00af" width="500" height="300">

### 상태가치함수(v : State Value Function)
  
앞서 반환값(G)으로 에피소드 하나에 대한 가치를 측정할 수 있었다면, 상태가치함수는 환경 전체에 대한 가치를 측정한다. 즉, 상태전이확률을 같이 고려한다.

|  | 측정 대상 | 특징 | 감가율 r | 상태 전이 확률 P |
| --- | --- | --- | --- | --- |
| 반환값 G | 에피소드 | 합계 | 사용 | 미사용 |
| 상태 가치 함수 v | 전체 환경 | 기댓값 | 사용 | 사용 |

####  상태가치함수 수식

$$
\begin{aligned}
v(s) &= E[G_t|S_t=s]&.. 1\\
 &= E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]&.. 2\\
 &= E[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + ...|S_t = s]&..3\\
 &= E[R_{t+1} + \gamma G_{t+1} | S_t = s]&..4\\
 &= E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]&..5
\end{aligned}$$

1. 타임스텝 t가 상태 s에 있을 때의 반환값의 기댓값으로 정의한다.
    아래 그림과 식을 보면 이해하기 쉽다.
   <img src="https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/3b9cfc10-f965-4460-a195-99b3daa9497e" width="220" height="150">
3. 반환값을 구하는 식을 그대로 집어넣는다.
4. 다음 타임스텝의 반환값을 감가율로 묶는다.
5. 이것을 다음 타임스텝의 반환값으로 대치한다.
6. 마지막 수식은 반환값 대신에 상태가치함수를 집어넣은 것이다. 반환값을 상태가치 함수로 대체할 수 있는 이유는 반환값에 대한 기대값을 구하면 상태 가치함수를 구하는 것과 같기 때문이다. 

# 2. 벨만 방정식(Bellman Equation)

벨만 방정식은 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계를 나타낸다.
그저위에서 본 상태가치함수를 수식을 좀 더 일반화하여 프로그래밍이 가능한 형태로 만든 것이다.
이를 **벨만 방정식(Bellman Equation)** 이라고 한다. 
강화학습에서는 프로그래밍으로 가치를 구하기 위해 이 방정식을 많이 사용한다. 

벨만 방정식은 **일반적으로 기댓값을 시그마 기호를 사용한 수열의 합으로 표현하며 현재상태와 다음상태의 관계**로 나타낸다. 

$$
\begin{aligned}
v(s) &= E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]&..1\\ 
&=R_{t+1} + \gamma E[v(S_{t+1})|S_t =s]&..2\\ 
&= R_{t+1} + \gamma\sum_{s' \in S}P_{ss'}v(s')&..3
\end{aligned}$$

1. 앞에서 본 상태가치함수
2. 상수를 기댓값에서 빼줌
3. 기댓값을 수열의 합과 다음 상태에서의 상태 가치 함수로 나타낸 것.

이 마지막 식을 **벨만 방정식**이라 부른다.

![타임스텝별로 고려해야 하는 상태](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/c0423288-be65-4c8b-98ee-5c4e51ac6564)



위와 같은 상황에서 벨만 방정식을 그대로 적용해 직접 가치를 계산하는 것은 쉬운 일이 아니다. 벨만 방정식을 사용하는 이유는 프로그래밍을 통해 문제를 해결하기 위함이다.

간단한 네트워크로 구성된 라우팅 문제의 경우 순차적으로 반환값을 구해 상태가치함수를 계산할 수 있다.
