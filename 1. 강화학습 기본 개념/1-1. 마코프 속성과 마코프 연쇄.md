## 목차
1. [강화학습이란?](#1.-강화학습이란?)
2. [확률과 확률과정](#2.-확률과-확률과정)

# 1. 강화학습이란?

강화학습(RL, Reinforcement Learning)이란 적절히 설계된 **보상 체계**를 활용해 에이전트가 긍정적인 행동을 할 수 있도록 에이전트 행동을 제어하는 정책을 찾아내는 최적화 기법

머신러닝은 크게 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning), 강화학습으로 구성된다. 지도학습은 정답을 알고 있는, 즉 y값이 있는 데이터로 학습하며, 비지도학습은 정답이 없이 주어진 데이터로만 학습한다. 강화학습은 정답이 주어진 것이 아니며 "보상"을 통해 학습한다.

강화학습을 통해 스스로 학습하는 컴퓨터를 "에이전트"라고 한다. 에이전트는 환경에 대해 사전지식이 없는 상태에서 학습을 한다. 그리고 그 환경으로부터 보상을 받으며 이 보상을 통해 에이전트는 어떤 행동이 좋은 행동인지 간접적으로 알게 된다.

에이전트는 자신의 행동과 그 결과인 보상을 통해 학습하며 어떤 행동을 해야 보상을 많이 얻게 될지를 알게 된다. 강화학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상을 최대화하는 "최적의 정책(행동양식)"을 학습하는 것이다.

보상을 꼭 양수로 설정할 필요는 없다. 안 좋은 행동을 했을 때는 음수의 보상, 벌점을 줄 수 있다. 적절한 상벌을 통해 에이전트를 잘 학습시키는 게 중요한 문제이다.

정리하자면, **에이전트(Agent)** 는 **정책(Policy)** 에 따라 어떤 **환경 (Environment)** 에서 특정 **행동(Action)** 을 한다. 그러한 행동에 따라 환경의 **상태(State)** 가 바뀌고 상태가 긍정적으로 바뀌 었는지 부정적으로 바뀌었는지에 따라 **보상(Reward)** 을 받는다.

* **강화학습 구성요소**
![강화학습 구성요소](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/f5f981f4-e0e6-4c74-8dad-cb7366a48033)

행동, 상태의 종류가 적은 경우에는 계산을 통해 최적의 정책을 찾을 수 있으나 상태의 종류가 많아질수록 최적의 정책을 찾는 계산 급속하게 증가한다. 이럴 때는 인공신경망 활용한다.


# 2. 확률과 확률과정

강화학습을 이해하기 위해 일단 확률을 알아야 한다. 확률적(stochastic)이라는 것은 무작위적(random)이다라고 이해할 수 있다.


### 확률 과정(Stochastic Process)

확률 과정이란, 시간의 흐름에 따라 확률적(무작위적)으로 움직이는 상태를 말한다.
{ ${X_t}$ } 로 집합으로 표현한다. X는 랜덤변수, t는 시간이다.

이런 확률 과정이라는 개념을 만들어 수학적으로 현상을 표현하면 프로그래밍을 통해 쉽게 문제 해결이 가능해진다. 

### 마르코프 속성(Markov Property)

마코프 속성은 확률과정의 특수한 형태로, memoryless, 즉 **과거에 일어났던 일들과 무관하게 현재의 상황만이 미래의 상황에 영향을 미친다** 는 것을 의미한다.

이렇게 과거의 일을 무시하고 현재만을 고려하면 문제를 풀기 훨씬 수월해진다. 마코프 속성을 조건부 확률로 나타내면 아래와 같다. St는 t시점에서의 상태를 의미한다.

$P[S_{t+1}|S_t] = P[S_{t+1}|S_1, ..., S_t]$

### 마코프 연쇄(Markov Chain)

마코프 체인(Markov Chain)은 마코프 속성을 지닌 시스템의 시간에 다른 상태 변화를 나타낸다. 즉, 과거와 현재 상태가 주어졌을 때, 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정되는 환경을 말한다.

- 시간이 이산적일 때 → Markov Chain 이라고 하며
- 시간이 연속적일 때 → Markov Process 라고 한다.
