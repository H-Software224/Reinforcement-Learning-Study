# 다이내믹 프로그래밍

**최적화 이론**으로 문제를 나눠서 해결할 수 있는 조건

1. Optimal Substructure, 최적의 하부 구조
    
    큰 문제를 작은 문제로 나누고 작은 문제에 대한 최적의 해답을 찾으면, 그 해답을 통해 큰 문제에 대한 해답을 찾을 수 있다는 것이다.
    
2. Overlapping Subproblems, 겹치는 하위 문제
    
    한 번 발생한 작은 문제는 반복해서 발생하므로 한 번 찾은 작은 문제에 대한 해답은 계속해서 재사용할 수 있다는 것이다.
    

→ MDP는 이 두 조건을 만족한다. 따라서 MDP 문제를 작은 단위로 나눠서 해결하는 가장 대표적인 방법인 **다이내믹 프로그래밍**을 사용할 수 있다. 

다이나믹 프로그래밍은 최적화 이론을 사용해 문제를 보다 쉽게 해결할 방법을 제시하는 대표적인 알고리즘이다. MDP와 같은 최적화 문제는 대부분 복잡한 환경으로 구성된 경우가 많다. 앞서 살펴본 예제는 몇 번의 계산으로 정답을 찾을 수 있지만, 네트워크의 노드가 수백 개가 되는 문제는 해결하기 어렵다.

### 모델 기반 환경에서 MDP 문제를 다이내믹 프로그래밍으로 풀기

다이나믹 프로그래밍(DP : Dynamic Programming)은 환경에 대한 모든 정보를 알고 있는 **모델 기반 방법론**이다. S, A, P, R, r 모두 알고 있으며, $v_\pi$와 $v^*$ 그리고 $\pi^*$를 계산할 수 있다.

그럼 이제 MDP를 다이나믹 프로그래밍을 사용해 해결해보자. DP는 정책평가를 통해 먼저 $v_\pi$를 계산하고, 다음으로 정책제어를 통해 $v^*$ 와 $\pi^*$를 계산하고 정책을 갱신한다.

먼저 정책 평가를 진행한다. 고정된 정책을 사용해 다음 타임스텝만을 고려해 가치함수를 계산한다. 그리고 현재 타임스텝의 가치를 갱신한다. 이 과정을 반복하면서 각 상태의 가치를 계산한다. 이 과정을 무한히 반복하면 MDP의 진짜 가치 함수의 값을 계산할 수 있다는 사실이 수학적으로 증명되었다.

고정된 정책에 따라 계산한 가치함수를 사용해 탐욕적으로 정책을 선택해 현재 정책을 갱신하는 것이 **정책제어**다. 이렇게 반복적인 정책평가, 정책제어로 최적의 정책을 찾아나간다.

이제 그리드월드 예제를 통해 자세히 알아보자.

그리드월드는 바둑판처럼 정사각형 으로 나누어진 환경에서 에이전트가 목적지를 찾아가는 게임이다. 예제 게임은 모두 16개의 상태를 가지고 있다. 게임에서 에이전트는 목적지(2개)를 제외한 나머지 14개 상태에 무작위로 위치하게 된다. 게임의 목적은 최단 거리로 에이전트가 목적지를 찾아가도록 정책을 설정하는 것이다.

- 상태 전이 확률을 1로 가정한다.
- 타임스텝이 진행됨에 따라 보상은 -1을 받는다.
- 동작은 상/하/좌/우 4개이며, 초기 정책은 각 행동별로 균등하게 0.25로 설정한다.

![그리드월드](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/4fc7c2a3-6cfc-467e-ba79-81b1413f2156)

우선 **정책 평가**를 먼저 진행하자.

k=0 좌측 상단과 우측 하단이 그리드월드의 목적지다.

k=1 하나의 타임스텝이 진행됨에 따라 모든 상태가 목적지를 제외하고 -1로 초기화된다.

k=2 타임스텝 2에서 좌표(0,1) 상태가치는 -1.75가 된다. 이를 계산하는 과정은 아래와 같다.

-1.0 + (0.0*0.25 -1.0*0.25 -1.0*0.25 -1.0*0.25) = -1.75

맨 앞 -1 은 현재 상태의 가치이다. 초기 정책은 모든 행동에 대한 확률이 0.25이기 때문에 각 상태 값에 정책을 곱해서 더해주면 다음스텝에서의 상태 가치를 구할 수 있다.

이런식으로 계속 계산하면 상태 가치를 계속해서 업데이트할 수 있다.

<img width="553" alt="그리드월드2" src="https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/2365c117-fbb3-4a72-8a86-d2c3698e06f0">


이제 **정책 제어**하는 법을 알아보자.

각 행동별로 0.25 확률로 초기화된 정책을 업데이트하고자 한다. 일단 위처럼 정책 평가를 반복적으로 수행해서 그리드별로 가치를 충분히 계산한다. 

정책 제어는 그리드에서 행동별 확률을 새로 설정하는데, 이때 탐욕적인 방식을 사용하려면 가장 가치가 큰 그리드로 이동하도록 정책을 설정한다. 앞선 예제에서는 -14의 가치를 가진 그리드가 가장 가치가 크기 때문에 각각 이동확률을 0.5로 동일하게 설정하고 나머지 그리드로 갈 확률은 0으로 설정한다. 이렇게 정책을 업데이트하는 것이다.

다이내믹 프로그래밍에서는 정책평가와 정책제어를 반복적으로 충분히 수행한다면 결국에는 최적가치와 정책을 찾을 수 있다.
