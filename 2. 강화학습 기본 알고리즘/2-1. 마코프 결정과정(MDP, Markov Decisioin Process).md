# 마코프 결정과정(MDP, Markov Decision Process)

마코프 결정과정(MDP, Markov Decision Process)은 앞서 본 MRP(마코프 보상과정)에 행동(A: Action)과 정책($\pi$: Policy)이 추가된 개념이다. 

- **MRP의 목적**
    - **에피소드나 환경전체의 가치를 계산**
    - 에이전트는 타임스텝에 따라 상태전이확률(P)에 영향을 받으며 자연스럽게 이동한다.

- **MDP의 목적**
    - **환경의 가치를 극대화 하는 정책을 결정하는 것**
    - MDP에서는 에이전트(Agent)라는 새로운 개념이 등장한다. MDP에서는 에이전트가 취한 행동과 상태 전이 확률의 영향을 동시에 받아 환경의 상태가 바뀐다.
    - MDP에서 에이전트는 정책($\pi$)에 따라 행동(A)을 하며 상태(S)는 에이전트가 취한 행동과 상태전이확률(P)에 따라 바뀌게 된다.

## MDP의 구성요소

### 1. **상태(State)의 집합($S$)**

  상태 $S$ 는 에이전트가 관찰 가능한 상태의 집합이다. 상태라는 것은 에이전트가 갈 수 있는 상황이라고 생각하면 쉽다. 뒤에서 나올 그리드월드 예제에서의 상태 집합은 각 좌표이다. 

  어떤 시점 $t$에서 상태 $S_t$ 는 정해져 있지 않다. MDP에서 상태는 시간에 따라 확률적으로 변한다.
  
### 2. **상태 전이 매트릭스($P$)**

  *상태 전이 매트릭스* 또는 *상태 변환 확률*, *환경의 모델* 이라고도 부른다.
  에이전트가 어떤 행동을 취하면 에이전트의 상태는 변할 것이다. 그러나 항상 원하는 곳으로 가지 못 할 수도 있다. 환경의 영향을 받아 확률적인 요인이 들어간다. 이것을 수치적으로 표현한 것이 상태 변환 확률이다.

   $P^a_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$

  수식은 **시간 $t$에서 상태가 $s$ 였을 때 $a$ 행동을 할 경우 시간 $t+1$ 에서 상태가 $s'$ 로 변할 확률** 을 의미한다. 이 값은 에이전트가 알지 못하는 값으로 환경의 일부이다.
        
### 3. 보상함수($R$)

  보상은 에이전트가 학습하는 정보로, 환경이 에이전트에게 주는 정보이다. 에이전트가 어떤 행동을 했을 때 받을 보상을 수식을 나타내면 아래와 같다.
        
   $R_s^a = E[R_{t+1}| S_t = s, A_t = a]$ 
  
  즉, 시간 $t$에서 상태가 $s$ 였을 때 $a$ 행동을 할 경우 시간 $t+1$ 에서 받는 보상의 기댓값을 의미한다. 위 상태 전이 매트릭스에서 봤듯이, 에이전트가 a라는 행동을 하더라도 항상 똑같은 상태로 이동하지는 않는다. 환경에 영향을 따라 확률적으로 다른 상태로 이동하는 것이기 때문에 이를 기댓값으로 표현하는 것이다.

  보상함수에서 주의해서 보아야 할 점은, 행동은 $t$ 시점에 하지만 보상을 받는 건 $t+1$ 이다. 에이전트가 보상을 알고 있지 않고, $t+1$ 시점에 이동한 그 상태를 보고 환경이 에이전트에게 보상을 주는 것이기 때문이다. 따라서 $R_{t+1}$ 와 같이 표현한다. 
        
### 4. 감가율($\gamma$)

   감가율 또는 할인율이라고 부른다. 가까운 보상일수록 더 큰 가치를 주기 위해 사용한다. 10년 뒤에 1억을 받는 것과 지금 당장 1억을 받는 것에는 차이가 있다. 현재가치로 환산해서 비교해야 한다. 이때 사용하는 것이 감가율(할인율)이다.

   미래에 받을 보상을 현재의 가치로 환산한다면 그 크기가 작아질 것이다. 할인율 $\gamma$는 0와 1사이의 값을 가지기 때문이다.

  만약 현재 시간인 $t$로부터 시간 $k$가 지난 후에 보상을 $R_{t+k}$ 만큼 받는다면, 이를 현재가치로 환산하면 어떻게 될까? 현재로부터 시간이 k만큼 지났기 때문에 미래에 받을 보상은 $\gamma^(k-1)$ 만큼 할인된다. 따라서 할인율을 고려한 보상의 현재 가치는 $\gamma^(k-1)R_{t+k}$ 이다.
   
### 5. 행동(Action)의 집합($A$)

  에이전트가 상태 $S_t$ 에서 할 수 있는 가능한 행동의 집합을 의미한다. 시간 $t$에서 행동 $a$를 했다면 $A_t = a$ 이렇게 표현할 수 있다. t라는 시간에 에이전트가 어떤 행동을 할지 정해져 있는 것이 아니므로 A와 같이 대문자로 표현한다. 즉, 확률변수이다. 그리드월드 예제에서 에이전트가 할 수 있는 행동은 up, down, left, right 로 네 가지 이며, $A = \{up, down, left, right\}$ 이렇게 표현할 수 있다. 
   
### 6. 정책 함수($\pi$)

정책은 모든 상태에서 에이전트가 할 행동이다. 정책은 하나의 상태에 대해 단 하나의 행동만 할 수도 있고, 확률적으로 $a_1 = 10%, a_2 = 90%$ 와 같이 나타낼 수도 있다.
MDP는 MRP의 상태전이매트릭스와 보상함수에 ‘행동’이라는 조건이 더 추가됐다. 따라서 상태 전이 매트릭스와 보상함수 또한 행동을 함께 생각해줘야 한다. MDP에서 취할 수 있는 행동의 개수는 상태와 마찬가지로 유한하다. 정책에 대해서는 아래에서 더 자세히 알아보자.



## MDP에서 정책이란?

MDP 정책은 아래와 같이 표현한다.
    
$$
\pi(a|s) = P[A_t = a | S_t = s]
$$

MDP에서 정책이란 행동을 선택하는 확률이다. 따라서 정책에서 따라 행동한다는 것은 확률이 높은 행동을 할 가능성이 크다는 의미이다. 에이전트는 이 정책을 가지고 있어서 모든 상태에서 자신이 해야 할 행동을 알고 있는 것이다.
MDP에서 에이전트는 항상 정해진 길을 따라가진 않는다. 언제나 의외성이 존재한다. 이는 이후 나올 탐험(Explopration)과 연관이 있다.

![특정 상태가 될 확률](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/fc3342b1-400c-40a7-bcdd-bed6d2bd125f)


위를 보면 정책의 확률과 그쪽으로 이동할 확률을 곱해준 후, 다 더한다. 에이전트는 s2로 가는 a1과 s3로 가는 a2중 하나를 고를 수 있다. 에이전트가 정책에 따라 a1을 선택했다고 하더라도 반드시 s2로 이동하지 않는다. 바로 상태전이 확률에 영향을 받기 때문이다.

상태전이확률은 에이전트의 의지와 전혀 상관없는 환경에서 자연적으로 발생하는 환경이다. 따라서 정책에 대한 확률과 상태전이확률을 각각 곱해서 더해야 한다.

위 그림에서 볼 수 있듯, MRP나 MDP나 s1에서 s2로 이동할 확률은 동일하다. 그러나 MDP는 정책이라는 새로운 요소가 추가된 것 뿐이다. 이 덕분에 새로운 기능을 추가할 수 있다.

MDP에서 에이전트의 행동은 오로지 정책에 의해 결정되며, 정책은 시간에 따라 변하지 않는다. MDP도 마코프 속성을 가정하므로 현재상태에만 영향을 받는다.

- 정책을 고려한 상태 전이 매트릭스
    
$$
P_{ss'}^\pi = \sum_{a \in A} \pi(a|s)P_{ss'}^a
$$

- 정책을 고려한 보상 함수
$$
R_s^\pi = \sum_{a \in A}\pi(a|s)R_s^a
$$
    

# 상태 가치 함수

이렇게 MDP에서 중요한 정책에 대해 알아보았다. 강화학습으로 풀고 싶은 문제는 최적의, 최대의 보상을 얻기 위해 매 순간 어떻게 행동해야 할지에 대한 "최적 정책"인 것이다. 그럼 이 최적 정책은 어떻게 찾을 수 있을까? 바로 "가치 함수"를 통해 행동을 선택할 수 있다.

현재 어떤 상태에 있고, 앞으로 받을 보상들을 고려해서 좋은 선택을 해야 한다. 그 아직 받지 않은 보상들을 더해서 이를 최대가 되도록 하는 행동을 하면 된다.
정책도 매트릭스 형태의 조건부 확률이다. MDP에서 상태가 변한다는 것은 원래 가지고 있던 상태 전이 매트릭스와 정책의 영향을 동시에 받는다는 것이다. 따라서 행동에 따른 정책과 상태전이확률의 기댓값을 구함으로써 정책을 고려한 상태전이매트릭스를 구할 수 있다.
정책을 고려한 보상함수 또한 상태전이매트릭스와 동일하게 정책과 각 행동별 보상 함수의 기댓값을 통해 나타낸다.


### MRP와 MDP에서의 상태 가치 함수
MRP와 MDP의 상태가치함수가 어떻게 다른지 살펴보자. 

![MRP, MDP 상태가치함수](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/f2f7bdd8-a8e1-4fe4-9a1a-8442b08a24c5)


- MRP에서 상태가치함수

$$
\begin{aligned}
v(s) &=  E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]\\
 &= R_{t+1} + \gamma E[v(S_{t+1})|R_t = s]\\
 &= R_{t+1} + \gamma \sum_{s' \in S}p_{ss'}v(s')
\end{aligned}$$

    
- MDP에서 상태가치함수

$$
\begin{aligned}
v_\pi(s) &= E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t = s]\\
 &= \sum_{a \in A} \pi(a|s)(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi (s'))\\
 &= \sum_{a \in A}\pi(a|s)R_s^a + \gamma\sum_{a \in A}\pi(a|s)\sum_{s' \in S}P_{ss'}^a v_\pi(s')
\end{aligned}$$
    

여기서 눈여겨 봐야 할 점은 MRP와 MDP의 상태 가치 함수는 같다는 점이다. 두 값은 같지만, MDP에서 상태 가치 함수를 구할 때는 정책이라는 요소를 하나 더 고려한 것뿐이다.

$$
v(s) = v_\pi(s)
$$
