# 몬테카를로(MC) 방법

 반복된 무작위 추출(repeated random sampling)을 이용하여 함수의 값을 수리적으로 근사하는 알고리즘을 뜻한다.(출처 : 위키백과 - 몬테카를로 방법)

DP에서는 Model Based환경에서 정책을 평가하고 제어해 문제를 단위로 나누어서 순차적으로 계산한 뒤 최적의 정책을 찾을 수 있다. 하지만 모델의 구조를 정확히 파악할 수 없는 Model Free환경에서는 보상 함수, 상태 전이 확률, 다음 상태를 알 수 없기 때문에 몬테카를로 방법을 사용한다.

### 강화학습에서의 몬테카를로 방법
* DP는 전체 상태를 한 번씩 모두 실행하면서 각 State의 상태를 업데이트한 반면, 몬테카를로 예측에서는 **하나의 에피소드가 끝날 때 까지 실행하면서 경험을 모으고 누적한 경험을 평균내어 가치 함수를 근사적으로 계산한다.**

* 강화학습에서 몬테카를로 방법을 사용하기 위한 조건 : Agent가 동작하는 환경에 시작과 끝이 있어야 한다. 다른 말로 에피소드 단위의 환경이어야 한다.

* 선택한 정책과 환경이 가지고 있는 상태 전이 확률에 따라 Agent가 실제로 에피소드가 끝날 때까지 동작을 수행하면서 정보를 수집해야한다. 처음 한 번 에피소드를 수행했을 때는 True Value(참 가치 함수)와 차이가 크겠지만 수많은 에피소드를 반복하면 True value에 가까워질 것이다.

### MDP를 몬테카를로 방법으로 해결하기
MDP를 몬테카를로 방법으로 해결하기 위해 수식으로 표현해보자.

MDP에서 상태 가치 함수의 정의를 다시 상기시켜보면 1번식인 반환값의 기댓값으로 표현된다. 이는 모든 고정된 정책을 따라갔을 때 2-1부분인 모든 행동과 2-2부분인 모든 상태를 고려해서 가치를 계산한다. 즉 Model Based환경에서 가능한 방식이다.

MC에서는 3번 부분인 반환값의 평균을 상태 가치 함수로 사용한다. 상태 가치 함수는 아래와 같은 순서로 계산된다.
  1. 4번 - 먼저 에피소드가 끝날 때 까지 에이전트를 동작시킨다. 에피소드가 한 번 끝나면 카운트를 하나 증가시킨다.
  2. 5번 - 에피소드 동안 모은 반환 값을 모두 모아서 누적 반환값 변수인 S에 저장한다. 이때 MDP와는 달리 모든 행동과 상태를 고려하지 않고 에피소드 동안 Agent가 수행한 행동과 거쳐간 상태만 계산된다.
  3. 6번 - 누적 반환값을 누적 카운트로 나누어 평균을 구한다.

### 증분 평균(Incremental Mean)
**증분 평균**은 이전 타임스템까지 계산된 평균을 활용하여 새로운 값이 들어왔을 때 전체 평균을 빠르게 구할 수 있는 평균 계산 식이다. 아래와 같은 방식으로 유도된다.

1. 연속적으로 발생하는 $x$값에 대한 전체 평균을 구하려면 모든 $x$값을 더해서 발생 횟수로 나누어야 한다.
2. 가장 최근에 발생한 $x$값을  $x_k$로 분리하고 이전 타임스텝까지 발생한 값의 합을 수열의 합으로 묶는다.
3. 프로그래밍으로 계산하기 쉬운 형태로 바꾸기 위해 $k-1$로 곱해주고 나누어 준다.(계산의 편의를 위해 추가) , 3-1부분은 $k-1$까지만 $x$값을 합산했기 때문에 $k-1$번째 데이터까지의 합계가 된다.
4. 3-1부분은 이전 타임스텝까지 계산된 평균으로 생각할 수 있으므로 다음과 같이 바꿀 수 있다.
5. 계산을 위해 5번 식과 같이 식 변형을 한 뒤 프로그래밍하기 쉬운 형태로 변경하기 위해 6번 식으로 나타 낼 수 있다.

이제 증분 평균을 사용해서 MC를 프로그래밍할 수 있는 형태로 바꿔보자.
* 평균 Return : $V(s)$ ← $S(s)/N(s)$
* **증분 평균 Incremental Mean Return : $V(s)$ ← $V(s) + 1/N(s)(G_t-V(s))$**

원래 평균을 구할 때 누적 반환값을 누적 카운트로 나누었다. 이렇게 하면 과거에 수행했던 모든 에피소드에 대한 정보를 저장하고 있어야 하기 떄문에 시스템에 많은 부담을 주고 연산 속도가 느려진다.

증분 평균을 이용한다면  최신 에피소드에서 얻은 반환값인 $G_t$에서 이전 타임스텝까지의 반환값의 평균인 $V(s)$를 빼준 다음 에피소드 횟수로 나누어 주면 된다. 그리고 이 값을 $V(s)$랑 더해 주면 훨씬 빠르게 상태 가치 함수를 구할 수 있다.

$G_t$가 만약 $V(s)$와 같다면  $V(s)=V(s)$가 되어 더 이상 상태 가치 함수가 변하지 않는다. 이 상태가 True Value(참 가치 함수)가 되며 따라서 강화학습에서는 **$G_t$와 $V(s)$의 차가 최소가 되도록 하는 정책을 찾는 것**을 목표로 한다.

$$V(s) ← V(s) + 1/N(s)(G_t-V(s))$$

$$V(s)$ ← V(s) + \propto(G_t-V(s))$$

증분 평균 식에서 $1/N(s)$을 고정 상수 값인 $\propto$로 변경해서 사용할 수 있다. $\propto$는 $0<\propto<1$인 값을 사용한다. 이렇게 변형하면 프로그래밍하기가 훨씬 수월해진다. **수학적으로 완전한 식에서 하나 둘씩 프로그래밍 편의를 위해 새로운 값으로 대체**하는 것은 MC가 경험적으로 솔루션을 찾아가는 방법론이기 때문에 가능한 것이다.
