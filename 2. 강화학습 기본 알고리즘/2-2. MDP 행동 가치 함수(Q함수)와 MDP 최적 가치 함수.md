# MDP 행동 가치 함수(Q 함수)

MDP의 목적은 환경의 가치를 극대화하는 정책을 결정하는 것이다. 그리고 정책은 행동을 결정하는 확률이다.

상태 가치 함수는 상태를 중심으로 가치를 평가했다. 정책을 평가하기 위해서는 ‘행동’을 중심으로 평가해야 한다. 이것이 바로 **행동 가치 함수(Q : Action Value Function, Q함수)**이다.

![Q함수](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/ff5ae98a-035d-4729-82ba-81ee0a8b4e2f)


1번은 상태가치함수를 다시 기술한 것이다. 행동가치함수에서는 행동을 미리 선택했기 때문에 기댓값을 구할 필요가 없다. 따라서 1-1과 1-2 부분을 빼고 수식을 3번과 같이 기술할 수 있다.

$\pi(s',a')$ 부분이 추가된 것은 다음 상태에서의 보상을 정확히 계산하기 위해서는 정책과 상태 전이 확률 매트릭스를 곱해줘야 하기 때문이다.

Q함수는 선택할 수 있는 여러 행동 중 하나를 선택했을 때의 가치를 계산한다.



$$\begin{aligned}
v_\pi(s) = \sum_{a \in A}\pi(a|s)q_\pi(s, a)  ..1\\
q_\pi(s,a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s') ..2
\end{aligned}$$

행동 가치 함수는 내가 선택한 행동의 가치를 계산하는 함수이고, 상태 가치 함수는 특정 상태의 가치를 계산하는 함수이다.

1. MDP에서 하나의 상태에서 다른 상태로 이동하기 위해서는 상태 전이 매트릭스와 함께 행동을 선택할 확률인 정책을 함께 고려한다. 따라서 행동 가치 함수를 사용해서 상태 가치 함수를 구하기 위해서는 정책에 대한 기댓값을 구한다.
2. 행동 가치 함수는 어떤 행동을 했을 때 그 가치가 어떻게 되는지 계산한 것이다. 가치는 현 상태에서 즉시 받을 수 있는 보상 + 미래에 받을 수 있는 보상으로 계산한다. 미래 보상은 행동을 했을 때 이동한 상태에 따라 달라진다.
    
    $P_{ss'}^a$ → 상태 전이 매트릭스에서 모든 행동을 고려하는 것이 아니라 하나의 행동만을 고려한다.
    

MDP는 가치를 극대화 하는 것이 목표이다. 가치를 기반으로 정책을 평가해서 가치를 최대화하는 정책(최적 정책 : Optimal Policy)를 찾는 것이 최종적인 목적이며, 이것이 강화학습의 기초적인 개념이다.

# MDP 최적 가치 함수

이제 MDP의 최종목적을 달성하기 위해 최적 가치 함수(Optimal Value Function)에 대해 알아보자.

최적 가치 함수는 아래처럼 최적상태 가치함수와 최적행동 가치함수로 나눌 수 있다.

$$
v^*(s) = max_\pi v_\pi(s)
$$

$$
q^*(s,a) =  max_\pi q_\pi(s,a)
$$

1. 최적 상태 가치함수 : 여러 정책을 따르는 상태가치함수가 있을 때 가치를 최대로 하는 정책을 따르는 상태 가치 함수를 말한다.
2. 최적 행동 가치 함수 : 다양한 정책을 따르는 행동 가치 함수 중에서 가치를 최대로 하는 정책을 따르는 행동 가치 함수를 말한다.

MDP에서 최적 행동 가치 함수를 안다는 것은 가장 효율적인 행동을 선택할 수 있는 정책을 안다는 것과 같다. 따라서 최적 행동 가치 함수를 찾아내면 MDP 문제를 해결할 수 있다.

- 최적정책의 특성
    
  * $\pi^*  : \pi^* \ge \pi, \forall \pi$  

  * $v_{\pi*}(s) = v^*(s)$

  * $q_{\pi*}(s) = q^*(s)$
    

- 최적 정책을 나타내는 방법

$$
\pi^*(a|s)=\left\lbrace
\begin{array}{ll}
1 & \textrm{if a = argmax q(s,a)} 
\newline
0 & \textrm{otherwise}
\end{array}
\right.
$$
    
    
어떤 행동 a가 최적 행동 가치 함수의 최댓값을 반환하게 만드는 행동이라면 그 행동에 대한 정책은 1이 되고, 그렇지 않으면 0이 된다.
    
이때 정책은 행동을 선택할 수 있는 확률이므로 상태 s에서의 정책은 확률이 1로 설정된 행동을 반드시 선택하게 된다.
