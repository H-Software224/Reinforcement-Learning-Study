# TD와 SARSA
## 1. TD
MC는 상태 가치 함수를 계산하는 시점이 에피소가 완료된 후기 때문에 학습을 느리게 한다는 단점이 있다. 그래서 TD(Temporal Difference Learning)이라는 새로운 개념이 등장한다.


$$MC : V(s_t) ← V(s_t) + \propto(G_t-V(s_t))$$

$$TD : V(s_t) ← V(s_t) + \propto(R_{t+1}+\gamma V(s_{t+1})-V(s_t))$$

$$from \ G_t \ to \ R_{t+1}+\gamma V(s_{t+1})$$

MC에서는 $G_t$(반환값)는 하나의 에피소드가 끝났을 때 얻을 수 있는 값이다. 이를 효율적인 학습을 위해 하나의 타임스텝이 완료되면 얻을 수 있는 값으로 대체할 수 있다.

* $R_{t+1}$ : 다음 타입스텝에서 얻을 수 있는 가치
* $\gamma V(s_{t+1})$ : 계산(또는 추정)에 의해 얻을 수 있는 가치 


TD의 이해를 돕기 위해 DP,MC,TD를 시각화해서 표현해 보자.

![KakaoTalk_20240115_044616589](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/67497047/fa875f15-733c-42de-81ab-11fdb0f6b641)


* DP는 하나의 상태에서 갈 수 있는 모든 상태를 고려해 가치를 계산하고 바로 정책을 평가(가치를 업데이트)한다. 그리고 이를 반복한다.
* MC는 에피소드를 따라가면서 가치를 계산하고 에피소드가 끝나면 한번에 정책을 평가한다.
* **TD는 하나의 행동을 선택했을 때 얻을 수 있는 가치만을 고려하고 바로 정책을 평가한다.** 그리고 이를 반복한다.
  
  -> DP의 짧고 잦은 반복 + MC의 하나의 행동을 선택해서 실행한다는 특징을 결합한 알고리즘이다. 그리고 에피소드가 완전히 끝나지 않아도 가치 함수를 계산할 수 있으므로 MC와 달리 끝이 정해지지 않은 환경(Nonterminating Environment)에서도 사용할 수 있다.

### Q함수
MC와 TD는 Model free환경에서 사용하므로 다음 상태를 알 수 없으며 어떤 상태로 갔을 때 가장 큰 가치함수를 구할 수 있는지 알 수 없다. 그래서 행동을 평가하기 위해 아래와 같은 Q함수를 사용한다.

$$q_\pi(s,a) = R_s^a + \gamma\sum_{s' \in S}{P_{ss'}^av_\pi(s')}$$

여기서 a는 특정 행동을 했을 때의 가치를 뜻한다. 즉 Q 함수는 상태 가치 함수에서 하나의 행동만 선택했을 때 얻을 수 있는 보상을 측정하는 것이다.

상태 가치 함수를 통해 Q함수를 얻기 위해서는 하나의 행동을 했을 때 갈 수 있는 모든 상태에서의 상태 가치 함수와 상태 전이 확률의 기댓값을 구하면 된다.

### TD에서의 정책 제어
DP에서는 정책을 업데이트 하기 위해서 다음 타입스텝의 모든 상태에서 상태 가치 함수를 구한 뒤 가장 큰값으로 이동하는 행동을 하도록 정책을 변경했다. 하지만 TD는 Model Free환경에서 사용되기 때문에 다음 타임스텝에 갈 수 있는 상태가 무엇인지 모른다.

$$\pi'(s) = \underset{a\in A}{\mathrm{argmax}}{Q(s,a)}$$
TD에서는 각각 행동을 모두 하면서 가장 큰 Q함수(행동 가치 함수)를 찾아 그와 관련된 행동을 하도록 정책을 수정한다. 즉 위와 같은 수식을 따른다.

몇 번의 타임스텝을 실행해서 이를 적용할 수 있는데 이것을 TD $(\lambda)$라 부른다 $\lambda$는 타임스텝을 의미하며 2타임스텝을 실행하면 TD(2)가 된다.

<br/>
<br/>
<br/>

## 2. SARSA
TD는 정책을 평가할 때는 상태 가치 함수를 사용하고 정책을 평가할 때만 Q함수를 사용한다. 하지만 Q함수로 정책을 평가하고 동시에 제어할 수도 있다.

$$TD : V(s_t) ← V(s_t) + \propto(R_{t+1}-\gamma V(s_{t+1})-V(s_t))$$

$$SARSA : Q(S,A) ← Q(S,A) + \propto(R_{t+1}+\gamma Q(S',A')-Q(S,A))$$

$$(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}) → S.A.R.S.A$$

* Q함수는 상태 $S_t$일 때 $A_t$행동을 하고 보상 $R_{t+1}$을 받는다. 다음 상태인 $S_{t+1}$에서 $A_{t+1}$행동을 하며 앞에서 했던 상황을 반복한다. 따라서 S,A,R,S,A가 연속된다 해서 이름이 SARSA라고 붙여졌다.
* SARSA에서도 정책 제어는 Q 함수를 최대로 하는 행동을 선택하도록 업데이트한다.
