# Q-Learning

### 1. On Policy와 Off Policy
- **On policy** : 정책을 평가하는 데 사용하는 정책과 정책을 제어하는 데 사용하는 정책이 모두 같음
    
    → 한 번 평가에 사용했던 경험을 나중에 재사용 하지 못함 + 정책을 항상 greedy(가장 큰 Q값을 가진 행동을 선택)하게 선택하기 때문에 다양한 정책을 적용하지 못하는 문제가 발생!
    
- **Off policy** : 평가와 제어에 각각 다른 정책을 사용함

  * 정책 평가 , sample 수집 → $\mu(a|s)$
  * 정책 제어, policy 갱신 → $\pi(a|s)$

### 2. 중요도 샘플링(Importance Sampling)
- 함수 $f(x)$의 기댓값을 계산 하고자 하는 확률 분포$p(x)$를 알고 있지만 $p$에서 샘플을 생성하기 어려울 때 사용.
- 비교적 샘플을 구하기 쉬운 확률 분포 $q(x)$에서 샘플을 생성하여 $p(x)$에서 $f(x)$의 기댓값을 생성


- 확률 변수 : {상, 중, 하} → $A$(Set of Actions)
- 확률 분포 : {0.3, 0.4, 0.3} → $\pi$(Policy)

위의 그림의 R1상태에서 비교적 정확한 기댓값을 구하기 위해서는 다음 상태로 이동하는 많은 행동을 관찰(샘플 : samples)해서 평균을 구해야 한다. 이 떄 새로운 항해 경로를 찾는 경우라고 생각하면 샘플을 구할 수 없기 떄문에 **기존의 데이터를 이용해야 한다.** 

기존 항로 데이터를 활용하면 확률 변수, 확률 분포뿐 아니라 많은 샘플을 얻을 수 있기 때문에 중요도 샘플링 이론을 적용해서 신규 항로에 알맞은 기댓값을 구할 수 있다.

$$\sum{P(X)f(x)} = \sum{Q(x)\left[\cfrac{P(X)}{Q(X)}f(x)\right]}$$
- $P(X)$ : 어떤 환경에서 변수 $X$의 확률 분포 $P$
- $Q(X)$ : 다른 환경에서 변수 $X$의 확률 분포 $Q$
- $f(X)$ : X의 함수, 어떤 함수도 가능($sin$, $cos$, $2x+1$ 등)
- $\sum{P(X)Q(X)}$ : 변수 $X$의 함수 $f(X)$에 대한 확률 분포 $P$의 기댓값

즉 중요도 샘플링으로 문제를 해결하기 위해서는 충분한 데이터를 가지고 있는 환경의 확률(Q)와 정답을 찾고자 하는 환경의 확률 분포(P)를 알고 있으면 된다.

MC와 TD를 이와 같은 중요도 샘플링을 사용해서 변경할 수 있는다. 수식은 아래와 같다.

                               **MC**
$$G_t^{\pi/\mu} = \cfrac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1})}  ... \cfrac{\pi(A_n|S_n)}{\mu(A_n|S_n)}G_t $$

$$V(S_t) ← V(S_t) + \propto (G_t^{\pi/\mu} - V(S_t))$$

                               **TD**

$$V(S_t) ← V(S_t) + \propto \left(\cfrac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))-V(S_t)\right)$$

$\mu$는 많은 경험을 통해 정보가 풍부한 환경에서의 정책이고 $\pi$는 알고 싶어하는 정책이다. MC를 사용해서 정책 $\pi$를 훈련하고 싶을 떄 정책 $\mu$를 사용해서 샘플을 얻고 중요도 샘플링을 통해 $\pi$를 훈련한다.

TD에서는 하나의 타임스텝만 실행되고 그에 대한 가치를 계산하는 반면, MC에서는 하나의 에피소드가 끝날 때까지 계속해서 샘플을 만들기 때문에 기댓값을 구하기 위해서 중요도 샘플링이 계속 곱해져야 하고 값이 심하게 왜곡될 수 있다. **따라서 MC에서 중요도 샘플링을 사용하는 것은 현실적으로 불가능하다.**

### 3. Q러닝(Q-Learning)

                           **Q-Learing에서 샘플링**
                               
$$ SARSA : Q(S,A) ← Q(S,A) + \propto(R_{t+1}+\gamma Q(S',A')-Q(S,A))$$

$$ **Q-Learning** : Q(S,A) ← Q(S,A) + \propto(R_{t+1}+\gamma \underset{a'}{\mathrm{max}}{Q(S',a')}-Q(S,A)) $$

**SARSA vs Q-Learning**
- SARSA : 경험을 쌓을 때 다음 행동을 결정하는 방식은 정책($\pi$)을 따라서 Q값을 계산
- Q-Learning : 경험을 쌓을 때 정책을 따라가는 것이 아니라 Q값을 max로 만드는 행동을 선택

  -> 중요도 샘플링을 사용하지는 않지만 평가(max)와 제어($\pi$)에 각각 다른 정책을 사용하므로 off policy 방법임

  일반적으로 Q-Learning이 SARSA보다 좋은 성능을 보여준다.

즉 Q-Learning에서는 별도의 고정된 정책을 사용하지 않고 정책 평가 과정에서 Q함수가 가장 큰 행동을 선택한 뒤 그 행동을 통해서 계산한 Q값으로 Q함수를 업데이트한다. 고정된 정책을 사용하지 않기 때문에 정책 제어 과정이 별도로 존재하지 않고 **정책평가와 제어가 동시에 이루어진다는 것**이 특징이다.
  
