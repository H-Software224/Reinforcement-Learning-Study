# 강화학습에 사용되는 다양한 용어

## 정책 평가와 정책 제어

* **정책 평가(Policy Evaluation)** : 상태 가치 함수 계산
* **정책 제어(Policy Control)** : 정책 변경


MDP에서 **정책 평가**는 상태 가치 함수를 구하는 것을 의미한다. 이는 어떤 정책을 따랐을 때 보상의 총 합계를 의미한다.

**정책 제어**는 정책을 변경하는 것이다. 정책을 평가해보니 설정한 정책으로 얻을 보상이 작거나 크다면 그에 따라 조정하는 것이다. MDP의 궁극적 목적은 가치를 가장 크게 만드는 것이니 반복적인 정책 제어 과정으로 최적의 정책을 찾는다.

정책평가 ↔ 정책제어는 상호 보완적으로 동작한다. 정책을 평가해서 정책이 얼마나 잘 설정되어 있는지 확인하고, 정책 제어를 통해 새로운 정책으로 갱신하며, 새로운 정책은 정책 평가로 다시 얼마나 효율적인지 평가할 수 있다.

정책평가와 정책제어가 구체적으로 어떻게 사용되는지는 이후 다이내믹 프로그래밍에서 확인하자.

## 모델 기반과 모델 프리

* **모델 기반(Model Based)** :  환경에 대한 모든 정보를 알고 있는 경우
* **모델 프리(Model Free)** :  환경에 대한 일부 정보만 알고 있는 경우 


![모델 기반, 모델 프리](https://github.com/LimSoYeong/Reinforcement-Learning-Study/assets/89073323/578ae753-2175-4bc6-9401-322a8ea436c3)

모델 기반은 환경에 대한 모든 정보를 알고 있다는 것이다. 여기에서 환경이란, MDP가 동작하는 모든 주변 상태를 말한다. 환경은 크게 상태, 상태 전이 확률, 보상, 행동, 감가율로 구성된다. 

모델프리는 블랙박스로 표시되며 입력된 행동과 상태에 따라 새로운 상태와 보상을 반환하는 환경이다.

이 둘의 가장 큰 차이점은 다음 상태를 알 수 있느냐, 없느냐다. 앞에서 본 예제에서는 다음 상태들이 화살표로 연결되어 있다. 복잡한 알고리즘을 사용하지 않고도 눈으로 다음 상태를 확인할 수 있다. 이때 사용하는 게 모델 기반 강화학습이다.

모델 프리의 경우 다음 타임스텝에서 에이전트가 갈 수 있는 상태를 알 수 없기 때문에 복잡한 알고리즘을 사용해 찾아내야 한다. 우리가 강화학습으로 해결해야 할 대부분의 문제는 바로 모델 프리 환경에 있다.
